\documentclass{proc}
\usepackage{url}
\linespread{1.2}

\begin{document}

\title{GlassBox: Fault-Tolerant Host Monitoring on Commodity Operating Systems}

\author{Brett Jia \hspace{1em} Jennifer Bi}

\maketitle

\section*{Abstract}

In modern computer systems, administrators, developers, and users often desire to monitor their operating systems for usage metrics, such as filesystem usage, CPU utilization, RAM utilization, and others. In other cases, systems administrators and developers may be interested in monitoring log files on production machines to gauge the health and execution of live applications. Monitoring can be done through automated software that report metrics to a metrics aggregator, or through a user manually running certain tools that probe the operating system for data. In either case, the system could be at risk of damage and compromise from buggy or malicious software and user negligence or error. To address this problem, we present an approach to running host monitoring tools in containers with a read-only view of the surrounding system, guaranteeing fault tolerance and protection to the host operating system.

\section*{1. Introduction}

Despite the best efforts of software designers and system administrators, a major weakness of system administration and system monitoring is the reliance on bug-free software and perfect user execution. While some system monitoring tools such as \texttt{ps} do not require elevated privileges and rely on reading deprivileged virtual filesystem objects exposed by the operating system kernel via the \texttt{procfs} mount, other tools such as \texttt{lsof} require superuser privileges to display full metrics on a system's open file descriptors by reading protected files in the same \texttt{procfs} mount. Especially for the case of tools requiring superuser privileges, any system administration or system monitoring task could potentially be destructive to the host system through malicious software or user error if incorrect commands and arguments are executed, resulting in the expenditure of hours and money to restore the system and its contents to a state prior to disaster.

Aside from monitoring of host system data and statistics, often system administrators and developers alike desire to monitor interesting files on a machine's filesystem for tracing the execution of a running application or troubleshooting software. Files such as \texttt{/var/log/messages} and application runtime logs provide a wealth of information about the health and execution of the system and live applications running on the host, and these logs can be used in a chain of standard Unix commands (such as \texttt{grep} and \texttt{sort}) to provide useful metrics and analysis. Similar to the risks of running system monitoring tasks, even accessing log files directly on the machine (whether using a privileged user or not) can jeopardize the health and stability of the host and the applications running on the host by accidentally crashing applications or starving the host of resources by spending too much computing resources on log file analysis.

An early solution to the problem of fault tolerance is to utilize virtual machines to guarantee isolation between processes \cite{garfinkel2003terra}. However, while virtual machines can indeed be used to provide an isolated environment for untrusted code execution \cite{wen2012virtualization}, recent performance comparisons between hypervisor-based virtualization and container-based (i.e. lightweight or kernel-based) virtualization show that virtual machines exhibit more overhead with certain workloads when compared to kernel-supported container mechanisms \cite{felter2014docker, morabito2015hypervisors}.

Instead of fault tolerance through hardware virtualization by way of virtual machines, recent work has emphasized ensuring fault tolerance through operating system virtualization \cite{soltesz2007container}. An early approach to operating system virtualization introduces a kernel interposition method to restrict an application's access to certain system calls (syscalls), as implemented in Janus \cite{goldberg1996janus} and later in MBOX \cite{kim2013mbox}. Other sandboxing mechanisms have been introduced, including new system calls to create kernel-supported process isolation called jails \cite{kamp2000jails}, new filesystem tools leveraging \texttt{mount} and \texttt{chroot} to restrict filesystem access \cite{prevelakis2001fmac}, namespace isolation to create kernel-supported process containers \cite{biederman2006namespaces, menage2007containers}, and control groups (cgroups) to limit resource consumption of a group of processes \cite{menagecgroups}.

Modern containers combine these operating system virtualization approaches to create lightweight isolated execution contexts. The industry standard choice for Linux container technology is Docker, which combines Linux \textit{cgroups}, \textit{namespaces}, \textit{capabilities}, and more using its custom container runtime, \textit{libcontainer} \cite{hykes2014libcontainer}. With the formation of the Open Container Initiative (OCI) in 2015 \cite{opencontainerinitiative}, the design and construction of container platforms became standardized with OCI's \textit{runtime-spec} and \textit{image-spec}, providing a framework for distributors to develop their own cross-compatible container implementations.

In our research, we present \textit{GlassBox}, a system to provide fault-tolerance and fault-containment semantics to host monitoring processes on commodity Linux operating systems. \textit{GlassBox} addresses the problems of host stability and fault tolerance that arise from executing buggy or compromised system tools and from user negligence and error when performing system administration tasks and application log monitoring through the use of modern container technologies. In the following sections, we first present related work on containers and existing container technologies, then discuss a theoretical view of the host resources a fault-tolerant host monitoring system should allow access to and the resources the system should protect, then discuss the prototype architecture and implementation of \textit{GlassBox}, and close with preliminary performance benchmarks and future work.

\section*{2. Related Work}

Current technologies allow for read-only containerization through built-in Linux support such as bind mounting. Bind mounts can be used in conjunction with containers such as Docker to provide stronger isolation guarantees and additional features \cite{dockerdoc}. However, configuring and setting up a container with read-only properties requires additional work. Furthermore, such configurations only grant access or protect access on the granularity of filesystems, while we would like more fine-grained protection of other resources, such as network access, CPU utilization, and RAM utilization.

Linux supports the creation of isolated environments through \texttt{chroot} jails and \texttt{mount} namespaces. While \texttt{chroot} changes the root directory for a process, \texttt{mount} namespaces allow for more flexibility and security. Bind mounts (\texttt{mount} command with \texttt{--bind} option) make a filesystem visible at a second mount point \cite{bindmount}. This kind of `symbolic link' allows for a specific view of the filesystem namespace. By bind mounting the root directory or \texttt{/proc}, host information becomes readable by a jailed process otherwise closed off from the filesystem.

Bind mounts can be used with Docker containers. One restriction is that the standard Docker command line interface (CLI) cannot be used to manage bind mounts. Thus, one would have to use the Docker CLI to manage the container itself but separately use Linux commands to manage the bind mount. A read-only container can also be achieved through read-only volumes. Volumes are created as new directories or pre-populated with data in Docker's storage directory. They are generally preferred for easy sharing, backup, migration, and management. However, not all of this functionality is relevant to our needs, and we hope to achieve the same read-only isolation for our specific workloads by combining bind mounts and syscall filtering.

Linux also supports syscall filtering via \texttt{seccomp-bpf} filters. \texttt{seccomp} is used in conjunction with namespaces and cgroups to implement containerization in Docker, LXC, etc. Currently, the container runtime runC (similar to libcontainer) uses \texttt{seccomp} to perform syscalls interposition \cite{opencontainerinitiative}, but their rule-based filtering does not allow for more advanced logic.

Host-monitoring applications isolated in read-only filesystems may also need a place to temporarily store the output of its monitoring. \texttt{tmpfs} can be used to store temporary data in RAM rather than on disk. Furthermore, the size can be limited to prevent a host-monitoring program from consuming too much memory. When combined with Docker, a \texttt{tmpfs} mount is persisted in host memory while the container runs, and removed when a container stops.

\section*{3. Research Methodology}

GlassBox provides an isolation mechanism for read-only host monitoring. We used bindfs/FUSE filesystem, OverlayFS filesystem, Linux tmpfs temporary file storage, the \texttt{iptables} utility, and Linux namespaces and capabilities in conjunction with the runC container runtime to grant read-only access to system information such as CPU load, process information, memory usage, and network traffic. GlassBox relaxes the isolation provided by containers like Docker. As a proof-of-concept it made sense to extend runC to implement our read-only isolation, rather than building our own container. Additionally, the runC component of GlassBox can be easily swapped out for a different container runtime such as LXC or rkt. 

Most host-monitoring tools, from simple commands that ship with Linux to complex graphical tools, all rely on a few Linux interfaces such as the \texttt{/proc} and \texttt{/sys} directories. To provide read-only access to these, we used a read-only bind mount. %% More words

GlassBox provides networking to containers via a bridge network. Each GlassBox instance has a network namespace to present the container its own network stack, routes, firewall rules, and network devices. Before the first container instance is launched, GlassBox creates the bridge network on the host and assigns an IP address. Then, for each container, we create a Linux Virtual Ethernet (veth) device pair in the host network namespace. The host end of the pair is added to bridge network in the host network namespace, and the guest end is moved to the guest network namespace and added as an interface. The veth pair acts as a tunnel between the bridge network and the guest network namespace. By default, the guest end of the veth pair is the only exposed interface in the container. Each guest-end of the veth pair is associated with an IP \texttt{192.168.10.<container id>}, where the id is a random integer between 0-255. The namespace object for the container lives at \texttt{/var/run/netns/runc<containerid>}. 

We create read-only behavior using \texttt{iptables} to restrict network packets. The \texttt{netfilter/iptables} utility uses a series of tables, each of which define firewall rules to filter or transform packets. To allow GlassBox to listen on the host's network traffic, we apply an iptables rule (\texttt{TEE}) that clones the packet and redirects it to our gateway IP. To prevent GlassBox processes from sending packets to (and potentially spamming) the host, we specify the iptables rule (\texttt{DROP}) to drop outgoing packets. 

Finally, Linux capabilities are required for the ability to monitor network traffic, specifically \texttt{CAP\_NET\_ADMIN} for listing all network interfaces, and \texttt{CAP\_NET\_RAW} to use raw sockets.

\section*{4. Evaluation}

We evaluated GlassBox on a range of host-monitoring and application performance monitoring tools. This includes Linux commands like \texttt{top} or \texttt{free}, more complex utilities like \texttt{sysstat} \cite{sysstat}, remote monitoring tools like \textit{GKrellM} \cite{gkrellm}, and graphical tools like \textit{Wireshark}, \textit{Cacti} \cite{cacti} and \textit{Nagios} \cite{nagios}. We also ensured that standard Unix tools (e.g. \texttt{awk, grep, vim, md5sum}) work correctly.

\section*{4. Hypothesis}

We hypothesize that monitoring tools will be able to run successfully and access host information in our container, while incurring some small overhead. The overhead will likely be due to syscall interposition and setup/teardown related to managing read access to host filesystems. We hypothesize that monitoring directly on the host do not get the benefits of protection of host resources, while monitoring from inside a Docker container is possible, but requires additional configuration for read-only access, and even then may be too restrictive for certain tools to function correctly.

\section*{5. Projected Research Schedule}

\textbf{3/5-3/11} perform initial tests on runC, explore runC codebase, design our container; create slides for project status report\\
\textbf{3/12-3/18} implementation: allow simple linux commands to access \texttt{/proc}\\
\textbf{3/19-3/25} implementation: support more complicated monitoring tools. begin draft of abstract\\
\textbf{3/26-4/1} implementation, continue writing abstract\\
\textbf{4/2-4/8} implementation, expand on abstract into the extended abstract\\
\textbf{4/9-4/15} test performance on monitoring tools, continue writing extended abstract\\
\textbf{4/16-4/22} more testing; adjust implementation based on performance testing, begin work on project presentation slides\\
\textbf{4/23-4/29} continue presentation slides; continue expanding on extended abstract into final report\\
\textbf{4/30-5/6} finish final report

\bibliographystyle{abbrv}
\bibliography{extended-abstract}
\end{document}
