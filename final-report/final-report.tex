\documentclass{proc}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{eso-pic}
\usepackage{picture}
\linespread{1.2}

\begin{document}

\title{GlassBox: Fault-Tolerant Host Monitoring on Commodity Operating Systems}

\author{Brett Jia \hspace{1em} Jennifer Bi}

\maketitle

\section*{Abstract}

In modern computer systems, administrators, developers, and users often desire to monitor their operating systems for usage metrics, such as filesystem usage, CPU utilization, RAM utilization, and others. In other cases, systems administrators and developers may be interested in monitoring log files on production machines to gauge the health and execution of live applications. Monitoring can be done through automated software that report metrics to a metrics aggregator, or through a user manually running certain tools that probe the operating system for data. In either case, the system could be at risk of damage and compromise from buggy or malicious software and user negligence or error. To address this problem, we present an approach to running host monitoring tools in containers with a read-only view of the surrounding system, guaranteeing fault tolerance and protection to the host operating system.

\section*{1. Introduction}

Despite the best efforts of software designers and system administrators, a major weakness of system administration and system monitoring is the reliance on bug-free software and perfect user execution. While some system monitoring tools such as \texttt{ps} do not require elevated privileges and rely on reading deprivileged virtual filesystem objects exposed by the operating system kernel via the \texttt{proc} mount, other tools such as \texttt{lsof} require superuser privileges to display full metrics on a system's open file descriptors by reading protected files in the same \texttt{proc} mount. Especially for the case of tools requiring superuser privileges, any system administration or system monitoring task could potentially be destructive to the host system through malicious software or user error if incorrect commands and arguments are executed, resulting in the expenditure of hours and money to restore the system and its contents to a state prior to disaster.

Aside from monitoring of host system data and statistics, often system administrators and developers alike desire to monitor interesting files on a machine's filesystem for tracing the execution of a running application or troubleshooting software. Files such as \texttt{/var/log/messages} and application runtime logs provide a wealth of information about the health and execution of the system and live applications running on the host, and these logs can be used in a chain of standard Unix commands (such as \texttt{grep} and \texttt{sort}) to provide useful metrics and analysis. Similar to the risks of running system monitoring tasks, even accessing log files directly on the machine (whether using a privileged user or not) can jeopardize the health and stability of the host and the applications running on the host by accidentally crashing applications or starving the host of resources by spending too much computing resources on log file analysis.

An early solution to the problem of fault tolerance is to utilize virtual machines to guarantee isolation between processes \cite{garfinkel2003terra}. However, while virtual machines can indeed be used to provide an isolated environment for untrusted code execution \cite{wen2012virtualization}, recent performance comparisons between hypervisor-based virtualization and container-based (i.e. lightweight or kernel-based) virtualization show that virtual machines exhibit more overhead with certain workloads when compared to kernel-supported container mechanisms \cite{felter2014docker, morabito2015hypervisors}.

Instead of fault tolerance through hardware virtualization by way of virtual machines, recent work has emphasized ensuring fault tolerance through operating system virtualization \cite{soltesz2007container}. An early approach to operating system virtualization introduces a kernel interposition method to restrict an application's access to certain system calls (syscalls), as implemented in Janus \cite{goldberg1996janus} and later in MBOX \cite{kim2013mbox}. Other sandboxing mechanisms have been introduced, including new system calls to create kernel-supported process isolation called jails \cite{kamp2000jails}, new filesystem tools leveraging \texttt{mount} and \texttt{chroot} to restrict filesystem access \cite{prevelakis2001fmac}, namespace isolation to create kernel-supported process containers \cite{biederman2006namespaces, menage2007containers}, and control groups (\textit{cgroups}) to limit resource consumption of a group of processes \cite{menagecgroups}.

Modern containers combine these operating system virtualization approaches to create lightweight isolated execution contexts. The industry standard choice for Linux container technology is Docker, which combines Linux \textit{cgroups}, \textit{namespaces}, \textit{capabilities}, and more using its custom container runtime, \textit{libcontainer} \cite{hykes2014libcontainer}. With the formation of the Open Container Initiative (OCI) in 2015 \cite{opencontainerinitiative}, the design and construction of container platforms became standardized with OCI's \textit{runtime-spec} and \textit{image-spec}, providing a framework for distributors to develop their own cross-compatible container implementations.

In our research, we present \textit{GlassBox}, a container-based system to provide fault-tolerance and fault-containment semantics to host monitoring processes on commodity Linux operating systems. \textit{GlassBox} addresses the problems of host stability and fault tolerance that arise from executing buggy or compromised system tools and from user negligence and error when performing system administration tasks and application log monitoring through the use of modern container technologies. In the following sections, we first present background on containers and existing container technologies, then discuss a theoretical view of the host resources a fault-tolerant host monitoring system should allow access to and the resources the system should protect, then discuss the prototype architecture and implementation of \textit{GlassBox}, and close with preliminary performance benchmarks and future work.

\section*{2. Containers}

Linux supports the creation of isolated filesystem environments through \texttt{chroot} jails and \texttt{mount} namespaces. While \texttt{chroot} changes the root filesystem directory for a process, \texttt{mount} namespaces allow for flexibility of what directory the process sees as the filesystem root and security of the overall filesystem on the host. Bind mounts (\texttt{mount} command with the \texttt{--bind} option) in particular make a subset of a host's filesystem visible at a second mount point \cite{bindmount}, with the option of mounting it as ``read-only.'' This kind of ``symbolic link'' allows for a specific view of the filesystem namespace. By bind mounting the root directory or \texttt{/proc}, host information becomes readable by a jailed process otherwise closed off from the filesystem. Indeed, bind mounts can currently be used with Docker containers to provide visibility to certain files on the host filesystem while keeping the containers' stong isolation guarantees \cite{dockerdoc}.

Host-monitoring applications isolated in read-only filesystems may also need a place to temporarily store the output of its monitoring. \texttt{tmpfs} can be used to store temporary data in RAM rather than on disk. Furthermore, the size can be limited to prevent a host-monitoring program from consuming too much memory. When combined with Docker, a \texttt{tmpfs} mount is persisted in host memory while the container runs, and removed when a container stops.

Linux also supports syscall filtering via \texttt{seccomp-bpf} filters, which are used in conjunction with other container primitives to implement containerization in Docker, LXC, etc. Currently, the container runtime \textit{runC} (built on its predecessor \textit{libcontainer} \cite{hykes2014libcontainer}) uses \texttt{seccomp-bpf} to perform syscalls interposition \cite{opencontainerinitiative}. The rule-based filtering of existing \texttt{seccomp-bpf} in Linux, however, does not allow for more advanced access control policy validation or syscall emulation \cite{seccompuserspace}.

Commercial container solutions such as Docker and rkt offer pre-built networking options to faciliate networking connectivity between the container and the host \cite{dockernetworking, rktnetworking}. The two general categories of network connectivity offered are \textit{host} and \textit{container}, both built on the network separation properties provided by Linux network namespaces. \textit{Host} network configurations set up the container to share the same network namespace as that of the calling process, effectively giving the container the same network access and network configuration as the parent process. \textit{Container} network configurations create a separate network namespace for the container, separating its connectivity from the network namespace of the parent process. In \textit{container} network configurations, a network bridge and virtual ethernet devices are used to connect the container's network with the host, effectively exposing the container as a new ``host'' connected to the host machine.

CPU and memory usage can also be restricted in existing container solutions through the use of Linux capabilities and \textit{cgroups} \cite{dockerconstraints}. As \textit{cgroups} function to group a set of processes under the same ``control group'' for broad resource control and process management \cite{menagecgroups}, container implementations such as Docker use \textit{cgroups} to fine-tune the memory and CPU-time allowances of the group of processes running in a container. Additionally, Linux capabilities can be used to give the container permission to modify its process scheduler directly in the host kernel, such as setting \textit{nice} values or use the Linux \textit{realtime} process scheduler.

\section*{3. Fault-Tolerant Host Monitoring}

We postulate that an ideal fault-tolerant host monitoring system should observe the following two characteristics:
\begin{enumerate}
    \item The system should be able to accurately view a subset of the host's state, both volatile (e.g. in-memory) and persistent (e.g. on-disk), as required by the information set the system is monitoring.
    \item The system's execution, both in normal and abnormal circumstances, must guarantee that stability of the host machine is maintained.
\end{enumerate}

An accurate view of the host's state is required for the host monitoring system to be useful. For example, the \texttt{top} program, which monitors system memory and CPU usage by process, requires access to volatile information to function correctly (in this case, the \texttt{/proc} virtual file system). It is easy to imagine that the \texttt{top} tool would not function correctly or be nearly as useful if \texttt{/proc} did not exist, or if reading from \texttt{/proc} yielded incorrect or invalid data.

On the other hand, a strong guarantee that the monitoring system's execution maintains the stability of the host machine is required to ensure fault-tolerance. We define the property of maintaining a host's \textit{stability} as ``preventing any modification of volatile and persistent data or usage of computing resources that could, now or in the future, alter the execution or responsiveness of the host or applications on the host.'' In other words, the monitoring system should ensure that it accesses memory and storage outside of the monitor in a strictly read-only fashion, should not send any signals or messages to existing processes, and should not starve the host of shared resources.

Traditionally, there is tension between these two requirements of accurate host monitoring and fault-tolerance. This tension can be easily exemplified by the tool \texttt{lsof}, which displays information about all of the host's open file descriptors by reading from the \texttt{/proc} filesystem. Running \texttt{lsof} with a non-superuser user can display some file descriptor information for processes running under the user, but is not permissioned to read information about processes running under other users. While executing \texttt{lsof} under a non-superuser user protects the host from harm if the \texttt{lsof} execution misbehaves or its binary has been compromised, it is clear that this protection is provided at the cost of losing access to certain information about the system. On the other hand, running \texttt{lsof} as a superuser would provide this information, but puts the host at significantly more risk if the \texttt{lsof} binary has been compromised.

\section*{4. GlassBox Implementation}

\begin{figure}
\includegraphics[width=0.5\textwidth]{architecture}
\caption{Architecture diagram}
\end{figure}

GlassBox provides an isolation mechanism that enables the creation of fault-tolerant host monitoring systems by providing host monitoring processes with a container that gives a read-only view of the host environment, targeted at exposing commonly monitored information such as CPU load, process information, memory usage, and network traffic. We configure this isolation mechanism in such a way that all requests to change the host's state, whether by modifications to the persistent data of the host filesystem or by modifications to volatile data in the host's memory outside the container, are blocked. Thus, this read-only container provides both of our criteria for a fault-tolerant host monitoring system: accurate view of the host state and strong guarantee of fault-tolerance.

In building our proof-of-concept, it made sense to extend runC to implement our read-only isolation, rather than building our own container, as runC is a well-established and well-tested example of a container runtime of the Open Container Initiative \cite{opencontainerinitiative}. Additionally, the runC component of GlassBox can be easily swapped out for a different container runtime such as LXC or rkt.

We discovered through the use of the Linux utility \texttt{strace} that most host-monitoring tools, from simple commands that ship with Linux (e.g. \texttt{ps} and \texttt{lsof}) to complex graphical tools (e.g. \textit{GKrellM} \cite{gkrellm}), all primarily rely on special Linux virtual filesystem mounts, such as the \texttt{proc} mount (on \texttt{/proc}) and the \texttt{sysfs} mount (on \texttt{/sys}). This implies that the commonly monitored host information (e.g. CPU load, memory, etc.) can be extracted from within the container as long as the container can read the host's \texttt{/proc} and \texttt{/sys} directories.

Traditionally, network traffic can be monitored with utilities such as \texttt{tcpdump}, which hooks into a ``network tap'' in the kernel's network stack using Berkley Packet Filters (BPF) \cite{bsdpacketfilter}. For a container to gain the same network information by running an unmodified \texttt{tcpdump}, the host's network packets would have to be accessible inside the container as well, while any packets produced by the container must not be routable to any host applications.

Finally, many host monitoring applications are interactive and present user interfaces that organize the system information collected (examples include \textit{Wireshark} and \textit{GKrellM}). To maximize usability, our container should enable a method of viewing these graphical applications, even when running inside the container.

Given these observations and goals, our implementation of GlassBox focuses on four features: \textit{proces isolation}, \textit{read-only filesystem}, \textit{read-only networking}, and \textit{graphical display}. In the following subsections, we discuss the details of our implementation that provide these four features.

\subsection*{4.1 Process Isolation}

Process isolation is critical to GlassBox to provide necessary fault-containment. Our implementation provides this by instructing runC to create a \textit{pid namespace}, in which all container processes will start and execute. This ensures that processes in the container are unable to communicate directly with processes outside the container using standard inter-process communication methods such as signals and shared memory.

\subsection*{4.2 Read-Only Filesystem}

Host monitoring processes inside GlassBox require access to the host's full filesystem to read from the \texttt{/proc} and \texttt{/sys} directories. We implement this through the use of the \texttt{bindfs} filesystem (built on FUSE) \cite{bindfs}, which presents a read-only userspace view of the host's full filesystem tree similar to a standard bind mount. By bind-mounting the host's root to the container root, we make the host root filesystem available inside the container, while by making the mount read-only, we prevent the container from modifying any of the host's files. Given access to the root file system, many tools like \texttt{top} can successfully collect statistics on the host, even when running inside GlassBox.

Host-monitoring tools may write temporary log files to the filesystem. GlassBox configures an ephemeral mount point at the \texttt{/tmp} directory (a location commonly used by applications to store temporary data) via \texttt{tmpfs}, and uses \texttt{overlayfs} to overlay a second \texttt{tmpfs} over the user's home directory (commonly used by applications to store personalized configuration data). This ensures that applications that must write to disk can continue to function correctly, while the writes to the filesystem are ephemeral. Figure 2 shows the setup. The \texttt{overlay\_home} directory is the writeable upper directory, \texttt{overlay\_work} is the work directory used for preparing files between atomic actions. We create an overlay of \texttt{overlay\_home} on top of the bind-mounted root filesystem, to present a unified view of the upper directory (which is writeable) and the underlying root directory tree (which is read-only).

% can go on to discuss reading log files for applications that want to monitor other processes

\begin{figure}
\includegraphics[width=0.5\textwidth]{fs_diagram}
\caption{Filesystem setup}
\end{figure}

\subsection*{4.3 Read-Only Networking}

GlassBox provides networking to containers via a bridge network. Each GlassBox instance has a network namespace to present the container its own network stack, routes, firewall rules, and network devices. Before the first container instance is launched, GlassBox creates the bridge network on the host and assigns an IP address. For each container, a Linux Virtual Ethernet (veth) device pair is created in the host network namespace. The host end of the pair is added to bridge network in the host network namespace, and the guest end is moved to the guest network namespace and added as an interface. The veth pair is a full duplex link, and thus acts as a tunnel between the bridge network and the guest network namespace. By default, the guest end of the veth pair is the only exposed interface in the container. Each guest-end of the veth pair is associated with an IP \texttt{192.168.10.<container id>}, where the id is a random integer between 0-255.

We create read-only behavior using \texttt{iptables} to restrict network packets. The \texttt{netfilter/iptables} utility uses a series of tables, each of which define firewall rules to filter or transform packets. To allow GlassBox to listen on the host's network traffic, we apply an iptables rule (\texttt{TEE}) that clones the packet and redirects it to our gateway IP. To prevent GlassBox processes from sending packets to (and potentially modifying the state of) the host, we specify the iptables rule (\texttt{DROP}) to drop outgoing packets. Thus, when multiple containers are connected to the same bridge network via veth pairs, each container cannot communicate with the host, nor with any other container.

Finally, runC provides configuration options for Linux capabilities to be set in a container. We set two capabilities by default, \texttt{CAP\_NET\_ADMIN} for listing all network interfaces, and \texttt{CAP\_NET\_RAW} to use raw sockets, which enable monitoring of network traffic.

One limitation of the networking setup is that the interfaces seen by GlassBox (for example, by running \texttt{ip addr}) are different from those seen by the host. We consider this discrepancy acceptable, since practically, host monitoring tools such as \texttt{tcpdump} and \textit{Wireshark} are interested in the network traffic rather than the the interfacs themselves. Since \texttt{TEE} clones the network packets, the IP source/destination appearing in packet captures still reflect the host interfaces.

\subsection*{4.4 Graphical Display}

GlassBox supports the execution of X11 applications inside the container and connects these processes with a display on the host. This enables users to launch full-graphical applications to visualize and interact with system information collected by host monitoring tools running from inside the container.

An X11 application reads from the environment variable \texttt{DISPLAY}, which contains three key pieces of information: \textit{hostname}, \textit{display number}, and \textit{screen number} \cite{xman}. When the hostname is specified, an X11 application tries to connect to that host's X server via TCP at the port number \texttt{6000 + <display number>}. When the hostname is not specified, the X11 application tries to connect to a Unix socket located at \texttt{/tmp/.X11-unix/X<display number>}.

GlassBox uses this X11 behavior to emulate an X server running inside the container and proxy the X11 connection to an X server running on the host. Before the container starts, GlassBox checks if a \texttt{DISPLAY} environment variable is set, and extracts the hostname. Assuming a \texttt{DISPLAY} variable exists, if there is no hostname, GlassBox assumes that the X server is running on the host and listening at the standard Unix socket location; otherwise, GlassBox assumes the X server is located at the host specified by \texttt{DISPLAY}. In either case, GlassBox will create a Unix socket file inside the container at \texttt{/tmp/.X11-unix/X<display number>} and uses the \texttt{socat} utility \cite{socatman} to bridge the Unix socket inside the container with the X server as defined by \texttt{DISPLAY}. A \texttt{DISPLAY} variable is then set inside the container to point to the newly-created Unix socket.

The X11 protocol also defines a way for a client to authenticate to a remote server using a \texttt{.Xauthority} file \cite{xsecurityman}, traditionally stored in a user's home directory. Without information stored in this file, an application inside the container cannot authenticate with the display server. GlassBox extracts the cookie stored inside the user's \texttt{.Xauthority} file that corresponds to the X server listed in \texttt{DISPLAY} and stores it in a \texttt{.Xauthority} file in the container's home directory. We use \texttt{overlayfs} to overlay this file over the home directory inside the container and avoid causing persistent modifications to the underlying filesystem data.

\section*{4. Evaluation}

We evaluated GlassBox on a range of host-monitoring and application performance monitoring tools. This includes Linux commands like \texttt{top} or \texttt{free}, more complex utilities like \texttt{sysstat} \cite{sysstat}, and graphical tools like \textit{Wireshark}. We have yet to try remote monitoring tools like \textit{GKrellM} \cite{gkrellm}, and other graphical tools such as \textit{Cacti} \cite{cacti} and \textit{Nagios} \cite{nagios}. We also ensured that standard Unix tools (e.g. \texttt{awk}, \texttt{grep}, \texttt{vim}, \texttt{md5sum}) work correctly.

We will also evaluate malicious or buggy software, to show that our host is isolated/protected from misbehaving programs.

\section*{5. GlassBox Performance}

%Include information about GlassBox experimental setup, including using tini as a lightweight userspace reaper to accurately track the container's process tree (since bindfs daemonizes)
We ran a preliminary memory profiler on GlassBox running in a Linux virtual machine. For our final results, we will run the profiler directly on a Linux machine, via Cloudlab. The memory profiler (Appendix A) traces memory usage across the launch of GlassBox, to the execution of \texttt{sleep 5} inside the container, and through the teardown of GlassBox. This execution spans across 15-20 executables that are called by GlassBox to set up the necessary namespaces, virtual network devices, X11 proxy, and others. runC itself requires 12 MB, and is the dominating memory cost. Other costs that remain consistent acrosss the execution of the container include the various structures we setup, such as the \texttt{bindfs} mount, and the \texttt{socat} connection between X11 server and client. Finally, many processes use memory during setup and teardown of the container.

We will also measure startup and execution overhead of GlassBox, using a runC container without GlassBox as a baseline. We expect these measurements to be somewhat implementation dependent on the container runtime, so it may be useful to measure GlassBox with other container runtimes such as LXC.

\section*{6. Conclusions}

%Include conclusions, mention there is more work to be done to improve the container and add additional features
With GlassBox, monitoring tools are able to run successfully and access host information in our container, with a high guarantee of fault tolerance. The implementation of GlassBox incurs some memory overhead, as shown by our profiling results. Even with these initial results, we expect that there is much future work that can be done to improve GlassBox and build new fault tolerant host monitoring systems.

\bibliographystyle{abbrv}
\bibliography{extended-abstract}

\clearpage

\section*{Appendix A}

\AddToShipoutPicture*{
    \put(.5\paperwidth,.5\paperheight){\makebox(0,0){\includegraphics[scale=1]{memdata.eps}}}
}

\end{document}
