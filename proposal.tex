\documentclass{proc}
\usepackage{url}
\linespread{1.2}

\begin{document}

\title{Project Proposal: A Container-Based Approach to Fault-Tolerant Host Monitoring on Commodity Operating Systems}

\author{Brett Jia \hspace{1em} Jennifer Bi}

\maketitle

\section*{Abstract}

In modern computer systems, users and developers often desire to monitor their operating systems for usage metrics, such as file system usage, CPU utilization, RAM, and others. Monitoring can be done through automated software that report metrics to a metrics aggregator, or through a user manually running certain tools that probe the operating system for data. In either case, the system could be at risk of damage and compromise from buggy or malicious software and user negligence or error. To address this problem, we present an approach to running host monitoring tools in containers with a read-only view of the surrounding system, guaranteeing fault tolerance and protection to the host operating system.

\section*{1. Introduction}

Despite the best efforts of software designers and system administrators, a major weakness of system administration and system monitoring is the reliance on bug-free software and perfect user execution. Some system monitoring tools such as \texttt{ps} rely on unprivileged reading virtual file system objects exposed by the operating system kernel, while other tools such as \texttt{lsof} require superuser privileges to display full metrics on a system's open file descriptors. Especially for the case of tools requiring superuser privileges, any system administration or system monitoring task could potentially be destructive to the host system through malicious software or user error if incorrect commands and arguments are executed, resulting in the expenditure of hours and money to restore the system and its contents to a state prior to disaster.

An early solution to the problem of fault tolerance is to utilize virtual machines to guarantee isolation between processes \cite{garfinkel2003terra}. However, while virtual machines can indeed be used to provide an isolated environment for untrusted code execution \cite{wen2012virtualization}, recent performance comparisons between hypervisor-based virtualization and container-based (i.e. lightweight or kernel-based) virtualization show that virtual machines exhibit more overhead with certain workloads when compared to kernel-supported container mechanisms \cite{felter2014docker, morabito2015hypervisors}.

Instead of hardware virtualization by way of virtual machines, recent work has emphasized creating fault tolerance with operating system virtualization \cite{soltesz2007container}. An early approach to operating system virtualization introduces a kernel interposition method to restrict an application's access to certain system calls, as implemented in Janus \cite{goldberg1996janus} and later in MBOX \cite{kim2013mbox}. Other sandboxing mechanisms have been introduced, including new system calls to create kernel-supported process isolation called jails \cite{kamp2000jails}, new file system tools leveraging \texttt{mount} and \texttt{chroot} to restrict file system access \cite{prevelakis2001fmac}, namespace isolation to create kernel-supported process containers \cite{biederman2006namespaces, menage2007containers}, and control groups to limit resource consumption of a group of processes \cite{menagecgroups}.

Modern containers combine these operating system virtualization approaches to create lightweight isolated execution contexts. The industry standard choice for Linux container technology is Docker, which combines Linux \textit{cgroups}, \textit{namespaces}, \textit{capabilities}, and more using its custom container runtime, \textit{libcontainer} \cite{hykes2014libcontainer}. With the formation of the Open Container Initiative (OCI) in 2015 \cite{opencontainerinitiative}, the design and construction of container platforms became standardized with OCI's \textit{runtime-spec} and \textit{image-spec}, providing a framework for distributors to develop their own cross-compatible container implementations.

In our research, we intend to explore the application of modern Linux container technologies to the area of system monitoring. In particular, we intend to address the problems of host stability and fault tolerance that arise from executing buggy or compromised system tools and from user negligence and error when performing system administration tasks. In the following sections, we list existing container and sandboxing technologies, our research approach, and our research timeline.

\section*{2. Literature Review}

In preparation for our research, we sought out existing literature on existing application isolation techniques. We discovered a diverse set of techniques used to create isolated and restricted environments for running untrusted code. Our findings are listed below in this section.

\begin{enumerate}
    \item Application-level sandboxing through programming language features, kernel interposition, or resource access control policies
    \item Simulating an operating system environment through virtualizing an operating system's runtime API
    \item Containerizing an execution environment through kernel-supported isolation of all resources required by a process
    \item Virtualizing hardware and isolating full operating systems through Type I and Type II virtual machine monitors
\end{enumerate}
\begin{enumerate}
    \item FreeBSD Jails \cite{kamp2000jails}
    \item File Monitoring and Access Control (FMAC) \cite{prevelakis2001fmac}
    \item Vx32 \cite{ford2008vx32}
    \item Native Client \cite{yee2009native}
    \item Apiary \cite{potter2010apiary}
    \item MBOX \cite{kim2013mbox}
    \item System sandbox (for Microsoft Windows) \cite{vokorokos2015sandboxMSWIN}
\end{enumerate}


\subsection*{2.2 File Monitoring and Access Control (FMAC)}

Prevelakis and Spinellis \cite{prevelakis2001fmac} present a filesystem sandboxing solution by isolating a process' access to the filesystem through a File Monitoring and Access Control (FMAC) tool, which presents a mirror of the local filesystem. The sandbox mounts the tool as a filesystem and uses \texttt{chroot} to restrict a process to only view the isolated file system. Access control policies can be set up to allow the application to perform certain read or write operations on select files in the mirrored filesystem, effectively protecting the local filesystem from unauthorized access and modification.

\subsection*{2.5 Apiary}

Potter and Nieh \cite{potter2010apiary} present an innovative approach to application compartimentalization through the use of containers. Apiary's application isolation and fault containment advances build upon existing operating system isolation features (e.g. Solaris zones and FreeBSD jails), but enables a holistic display system across all containers and inter-application communication and integration through restrictive, shared filesystems. Apiary also introduces the Virtual Layered File System, an efficient way of combining multiple layers of file systems that a container can view, but also require strict access control policies. Apiary's benchmarks show that the container technique presented adds minimal overhead to the normal operation of containerized applications.


\section*{3. Research Methodology}
We plan to implement an isolation mechanism for read-only host monitoring. First, we will define the set of resources which will be readable by applications running inside the container. For example, a reasonable monitoring tool would access information about CPU load, memory usage, disk usage, process information, which we want to make available. On the other hand, we may want to restrict access to other resources such as named pipes or network sockets.

Our container will relax the isolation provided by containers like Docker, but will be otherwise similar. It makes sense then to extend runC to implement our read-only isolation, rather than building our own container. Currently, runC uses \texttt{seccomp-bpf} filters to interpose on syscalls, but their rule-based filtering does not allow for more advanced logic. Our work will be to add support for syscall handling, and support for read-only access to the host filesystem. 

Finally we will evaluate our container on a range of host-monitoring and application performance monitoring tools. Our tentative list includes: linux commands like \texttt{top} or \texttt{free}, more complex utilities like \texttt{sysstat}\cite{sysstat}, remote monitoring tools like \emph{GKrellM}\cite{gkrellm}, and graphical tools like \emph{Cacti}\cite{cacti}, \emph{Nagios}\cite{nagios}. We will also ensure standard unix tools work (\texttt{awk, grep, vim, md5sum}). The main soft evaluation metric is ease of use compared to monitoring directly on the most and monitoring with a Docker container. The hard metrics include speed of instantiation, syscall speed, scalability with respect to memory and cpu usage (i.e., running multiple containers on the same host). 

In addition to well-behaving tools, we will run malicious programs to show that our container protects host resources from malicious behavior.

\subsubsection*{3.1 Required Hardware}
We will develop in VMs, but test and measure performance on x86-64 Linux machines (which will require access to the CLIC lab, and maybe CRF accounts).

\subsubsection*{3.2 Required Software}
We will use the same Linux distribution on development VMs as those in the Clic lab. Our other software is also open-source or free: standard system tools that ship with Linux, \emph{Cacti, Nagios, GKrellM, sysstat, Docker}.

\section*{4. Hypothesis}
We hypothesize that monitoring tools will be able to run successfully and access host information in our container, while incurring some small overhead. The overhead will likely be due to syscall interposition and setup/teardown related to managing read access to host filesystems. We hypothesize that monitoring directly on the host do not get the benefits of protection of host resources, while monitoring from inside a Docker container is possible, but requires additional configuration for read-only access, and even then may be restrictive.

\section*{5. Projected Research Schedule}
\textbf{3/5-3/11} perform initial tests on runC, explore runC codebase, design our container; create slides for project status report\\
\textbf{3/12-3/18} implementation\\
\textbf{3/19-3/25} implementation; begin draft of abstract\\
\textbf{3/26-4/1} implementation; continue writing abstract;\\
\textbf{4/2-4/8} implementation; expand on abstract into the extended abstract\\
\textbf{4/9-4/15} test performance on monitoring tools, continue writing extended abstract\\
\textbf{4/16-4/22} more testing; adjust implementation based on performance testing, begin work on project presentation slides\\
\textbf{4/23-4/29} continue presentation slides; continue expanding on extended abstract into final report\\
\textbf{4/30-5/6} finish final report




\bibliographystyle{abbrv}
\bibliography{proposal}
\end{document}


