\documentclass{proc}
\usepackage{url}
\linespread{1.2}

\begin{document}

\title{Project Proposal: A Container-Based Approach to Fault-Tolerant Host Monitoring on Commodity Operating Systems}

\author{Brett Jia \hspace{1em} Jennifer Bi}

\maketitle

\section*{Abstract}

In modern computer systems, users and developers often desire to monitor their operating systems for usage metrics, such as file system usage, CPU utilization, RAM, and others. Monitoring can be done through automated software that report metrics to a metrics aggregator, or through a user manually running certain tools that probe the operating system for data. In either case, the system could be at risk of damage and compromise from buggy or malicious software and user negligence or error. To address this problem, we present an approach to running host monitoring tools in containers with a read-only view of the surrounding system, guaranteeing fault tolerance and protection to the host operating system.

\section*{1. Introduction}

Despite the best efforts of software designers and system administrators, a major weakness of system administration and system monitoring is the reliance on bug-free software and perfect user execution. Some system monitoring tools such as \texttt{ps} rely on unprivileged reading virtual file system objects exposed by the operating system kernel, while other tools such as \texttt{lsof} require superuser privileges to display full metrics on a system's open file descriptors. Especially for the case of tools requiring superuser privileges, any system administration or system monitoring task could potentially be destructive to the host system through malicious software or user error if incorrect commands and arguments are executed, resulting in the expenditure of hours and money to restore the system and its contents to a state prior to disaster.

An early solution to the problem of fault tolerance is to utilize virtual machines to guarantee isolation between processes \cite{garfinkel2003terra}. However, while virtual machines can indeed be used to provide an isolated environment for untrusted code execution \cite{wen2012virtualization}, recent performance comparisons between hypervisor-based virtualization and container-based (i.e. lightweight or kernel-based) virtualization show that virtual machines exhibit more overhead with certain workloads when compared to kernel-supported container mechanisms \cite{felter2014docker, morabito2015hypervisors}.

Instead of hardware virtualization by way of virtual machines, recent work has emphasized creating fault tolerance with operating system virtualization \cite{soltesz2007container}. An early approach to operating system virtualization introduces a kernel interposition method to restrict an application's access to certain system calls, as implemented in Janus \cite{goldberg1996janus} and later in MBOX \cite{kim2013mbox}. Other sandboxing mechanisms have been introduced, including new system calls to create kernel-supported process isolation called jails \cite{kamp2000jails}, new file system tools leveraging \texttt{mount} and \texttt{chroot} to restrict file system access \cite{prevelakis2001fmac}, namespace isolation to create kernel-supported process containers \cite{biederman2006namespaces, menage2007containers}, and control groups to limit resource consumption of a group of processes \cite{menagecgroups}.

Modern containers combine these operating system virtualization approaches to create lightweight isolated execution contexts. The industry standard choice for Linux container technology is Docker, which combines Linux \textit{cgroups}, \textit{namespaces}, \textit{capabilities}, and more using its custom container runtime, \textit{libcontainer} \cite{hykes2014libcontainer}. With the formation of the Open Container Initiative (OCI) in 2015 \cite{opencontainerinitiative}, the design and construction of container platforms became standardized with OCI's \textit{runtime-spec} and \textit{image-spec}, providing a framework for distributors to develop their own cross-compatible container implementations.

In our research, we intend to explore the application of modern Linux container technologies to the area of system monitoring. In particular, we intend to address the problems of host stability and fault tolerance that arise from executing buggy or compromised system tools and from user negligence and error when performing system administration tasks. In the following sections, we list existing container and sandboxing technologies, our research approach, and our research timeline.

\section*{2. Related Work}
Current technologies allow for read-only containerization through built-in linux support such as bind mounting. Bind mounts can be used in conjunction with containers such as Docker to provide stronger isolation guarantees and additional features\cite{dockerdoc}. However, configuring and setting up a container with read-only properties requires additional work. Furthermore, such configurations only grant access or protect access on the granularity of filesystems, while we would like more fine-grained protection of resources.

Linux supports the creation of isolated environments through chroot jails and mount namespaces. While chroot changes the root directory for a process, mount namespaces allow for more flexibility and security. Bind mounts (\texttt{mount} command with \texttt{--bind} option) make a file system visible at a second mount point\cite{bindmount}. This kind of `symbolic link' allows for a specific view of the filesystem namespace. By bind mounting the root directory or \texttt{/proc}, host information becomes readable by a process otherwise closed off from the filesystem.

Bind mounts can be used with Docker containers. One restriction is that standard Docker command line interface cannot be used to manage bind mounts. Thus, one would use the Docker CLI to manage the container, and separately use Linux commands to manage the bind mount. A read-only container can also be achieved through read-only volumes. Volumes are created as new directories or pre-populated with data in Docker's storage directory. They are generally preferred for easy sharing, backup, migration, and management. However, not all of this functionality is relevant to our needs, and we hope to achieve the same read-only isolation for our specific workloads by combining bind mounts and syscall filtering.

Linux also supports sycall filtering via \texttt{seccomp-bpf} filters. Seccomp is used in conjunction with namespaces, cgroups to implement containerization in Docker, LXC, etc. Currently, the container runtime runC (similar to libcontainer) uses \texttt{seccomp} to interpose on syscalls\cite{opencontainerinitiative}, but their rule-based filtering does not allow for more advanced logic.

Host-monitoring applications isolated in read-only filesystems may also need a place to temporarily store the output of its monitoring. \texttt{tmpfs} can be used to store temporary data in RAM rather than on disk. Furthermore, the size can be limited to prevent a host-monitoring program from consuming too much memory. When combined with Docker, a \texttt{tmpfs} mount is persisted in host memory while the container runs, and removed when a container stops. 

%\subsection*{2.5 Apiary}

%Potter and Nieh \cite{potter2010apiary} present an innovative approach to application compartimentalization through the use of containers. Apiary's application isolation and fault containment advances build upon existing operating system isolation features (e.g. Solaris zones and FreeBSD jails), but enables a holistic display system across all containers and inter-application communication and integration through restrictive, shared filesystems. Apiary also introduces the Virtual Layered File System, an efficient way of combining multiple layers of file systems that a container can view, but also require strict access control policies. Apiary's benchmarks show that the container technique presented adds minimal overhead to the normal operation of containerized applications.


\section*{3. Research Methodology}
We plan to implement an isolation mechanism for read-only host monitoring. First, we will define the set of resources which will be readable by applications running inside the container. For example, a reasonable monitoring tool would access information about CPU load, memory usage, disk usage, process information, which we want to make available. On the other hand, we may want to restrict access to other resources such as named pipes or network sockets.

Our container will relax the isolation provided by containers like Docker, but will be otherwise similar. It makes sense then to extend runC to implement our read-only isolation, rather than building our own container. Our work will be to extend the rule-based filtering in runC with additional logic, to grant read-only access to resources needed for host-monitoring. 

Finally we will evaluate our container on a range of host-monitoring and application performance monitoring tools. Our tentative list includes: linux commands like \texttt{top} or \texttt{free}, more complex utilities like \texttt{sysstat}\cite{sysstat}, remote monitoring tools like \emph{GKrellM}\cite{gkrellm}, and graphical tools like \emph{Cacti}\cite{cacti}, \emph{Nagios}\cite{nagios}. We will also ensure standard unix tools work (\texttt{awk, grep, vim, md5sum}). The main soft evaluation metric is ease of use compared to monitoring directly on the most and monitoring with a Docker container. The hard metrics include speed of instantiation, syscall speed, scalability with respect to memory and cpu usage (i.e., running multiple containers on the same host). 

In addition to well-behaving tools, we will run malicious programs to show that our container protects host resources from malicious behavior.

\subsubsection*{3.1 Required Hardware}
We will develop in VMs, but test and measure performance on x86-64 Linux machines (which will require access to the CLIC lab, and maybe CRF accounts).

\subsubsection*{3.2 Required Software}
We will use the same Linux distribution on development VMs as those in the Clic lab. Our other software is also open-source or free: standard system tools that ship with Linux, \emph{Cacti, Nagios, GKrellM, sysstat, Docker}.

\section*{4. Hypothesis}
We hypothesize that monitoring tools will be able to run successfully and access host information in our container, while incurring some small overhead. The overhead will likely be due to syscall interposition and setup/teardown related to managing read access to host filesystems. We hypothesize that monitoring directly on the host do not get the benefits of protection of host resources, while monitoring from inside a Docker container is possible, but requires additional configuration for read-only access, and even then may be restrictive.

\section*{5. Projected Research Schedule}
\textbf{3/5-3/11} perform initial tests on runC, explore runC codebase, design our container; create slides for project status report\\
\textbf{3/12-3/18} implementation: allow simple linux commands to access \texttt{/proc}\\
\textbf{3/19-3/25} implementation: support more complicated monitoring tools. begin draft of abstract\\
\textbf{3/26-4/1} implementation, continue writing abstract\\
\textbf{4/2-4/8} implementation, expand on abstract into the extended abstract\\
\textbf{4/9-4/15} test performance on monitoring tools, continue writing extended abstract\\
\textbf{4/16-4/22} more testing; adjust implementation based on performance testing, begin work on project presentation slides\\
\textbf{4/23-4/29} continue presentation slides; continue expanding on extended abstract into final report\\
\textbf{4/30-5/6} finish final report




\bibliographystyle{abbrv}
\bibliography{proposal}
\end{document}


