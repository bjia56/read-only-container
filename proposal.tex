\documentclass{proc}
\usepackage{url}
\linespread{1.2}

\begin{document}

\title{Project Proposal: A Survey and Performance Comparisons of Techniques to Construct Restricted Computing Environments}

\author{Brett Jia \hspace{1em} Jennifer Bi}

\maketitle

\section*{Abstract}

In modern computer systems, users and researchers often need to run untrusted and potentially malicious code, yet would like to protect their systems from harm. This is often achieved by creating a restricted environment in which the untrusted code then executes safely. We seek to survey the current state of restricted computing technologies and present a comparison of various programs that provide this restricted computing environment.

\section*{1. Introduction}

Despite the best efforts of systems and hardware designers, a major weakness of modern day operating systems is the high level of trust given to user-level application code to allow programs to do what they are designed to do. A word processor is permitted to read and write files to the local filesystem, whereas a web browser is permitted to open sockets and make network connections. Without these capabilities, programs would be rendered useless, much to a computer user's dissatisfaction.
\newline\newline
While many software applications (such as word processors and web browsers) are now distributed through secure channels and feature cryptographic signatures from trusted entities, there are still instances where users want to run untrusted code, but still keep their local system safe from any malicious side effects. For example, web scripting languages such as JavaScript are present in a large majority of websites, which run the untrusted scripts immediately as a webpage loads. As users cannot be expected to disable JavaScript completely (thus rendering most websites unusable), browser designers are forced to design policies and mechanisms to isolate this untrusted code from the rest of the computer. In another case, a security researcher may want to run malware on a local computer for testing and study, but also seek to protect the computer from harm. Additionally, a system administrator may want to open up servers for public use, but limit the operations each user can perform to prevent or limit damage to the overall system.
\newline\newline
In each of these situations, the typical solution is to create a sort of restricted environment for unsafe programs to run. Creating such a restricted environment comes in many forms, from using tools and syscalls built into an operating system, to emulating the machine instructions of an application binary, to even using full-featured virtual machines. Selecting a tool from this wide buffet of options can be difficult, and may vary depending on the use case or software to be restricted. In our research, we seek to survey the current state of restricted computing technologies by discussing the different techniques employed, then present a comparison of various programs that provide this restricted computing environment in both objective, hard metrics as well as subjective, soft metrics.

\section*{2. Literature Review}

In preparation for our research, we sought out existing literature on existing application isolation techniques. We discovered a diverse set of techniques used to create isolated and restricted environments for running untrusted code.
\newline\newline
A prior survey of virtualization technologies for untrusted code execution from 2012 \cite{wen2012survey} noted that untrusted code is usually executed in one of four categories:
\begin{enumerate}
    \item Application-level sandboxing through kernel interposition and resource access control policies
    \item Simulating an operating system environment through virtualizing an operating system's runtime API
    \item Containerizing an execution environment through kernel-supported isolation of all resources required by a process
    \item Virtualizing hardware and isolating full operating systems through Type I and Type II virtual machine monitors
\end{enumerate}

\section*{3. Research Methodology}
We plan to evaluate a sample of widely-used sandboxes ranging from lightweight \texttt{chroot}-based jails to full-blown virtual machines. The hard (quantitative) metrics in our evaluation are: startup time, execution speed, syscall speed, memory overhead. The soft metrics are: hardware requirements, software/OS requirements, ease of setup and launch, granularity of configuration, configuration management. Ideally, the soft metrics will provide some context for the quantitative differences between the isolation techniques and reveal tradeoffs between functionality and performance. 
Based on our literature review, we found these sandboxes to be a representative sample of current sandboxing methods:\vspace{0.5em}
{\small
\begin{itemize}
\item minijail and firejail
\item QEMU
\item Native Client
\item MBox
\item Linux Containers
\item FreeBSD jail
\item Docker
\item VirtualBox running Debian guest
\end{itemize}
}
To collect hard metrics, we will run benchmarks that stress CPU and memory resources. We will then analyze the results and examine the implementations of the sandboxes to explain measurement trends. This may be repeated in an iterative process, for example running the same benchmark on increasingly stricter security configurations of each sandbox. As part of the measurement process, we will collect experience using these sandboxes to qualify our soft metrics.
\subsubsection*{Required Hardware}
Our tests will be performed on x86-64 Linux machines (which will require access to the CLIC lab, and maybe CRF accounts). We will use the same OS distribution when possible. In the case of FreeBSD jail running on FreeBSD or minijail running on Chrome OS, for example, we will account for OS differences in our analysis. 
\subsubsection*{Required Software}
We would want to use the standard SPEC CPU2017 benchmark. However, we are not sure if it is practical given its cost (\$1000). Other possibilities include using open-source benchmarks such as PXZ and sysbench. These were used in a 2014 performance study on containers and VMs by IBM Research ~\cite{felter2014docker}.\\
For measuring startup time and syscall time, we will use the \emph{perf} tool.
\section*{4. Hypothesis}
We hypothesize that there will be a small tradeoff between configurability and startup time. In particular, we anticipate Docker, VirtualBox VM, and QEMU to have startup overheads due to creation of a writable container layer for containers, and virtual disk creation for VMs. We expect execution time, memory overhead of containers and virtual machines to be close to native execution ~\cite{felter2014docker}. On the other hand, for sandboxes relying on syscall filtering methods like seccomp, we expect the main slowdown to be syscall speed due to interposition on a non-trivial fraction of those syscalls ~\cite{kim2013mbox}. While performance of containers and VMs on hard metrics have been studied, we hope to present measurements across a wider range of isolation techniques.
\section*{5. Projected Research Schedule}
\textbf{2/12-2/18} \\
\textbf{2/19-2/25}
\textbf{2/26-3/4}
\textbf{3/5-3/11}
\textbf{3/12-3/18} \\
\textbf{3/19-3/25}
\textbf{3/26-4/1}
\textbf{4/2-4/8}
\textbf{4/9-4/15}
\textbf{4/16-4/22}
\textbf{4/23-4/29}
\textbf{4/30-5/6}  




\bibliographystyle{abbrv}
\bibliography{proposal}
\end{document}


